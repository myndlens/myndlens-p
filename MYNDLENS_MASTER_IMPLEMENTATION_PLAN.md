# MyndLens Critical Implementation Plan

**Document Type:** Comprehensive Implementation Specification  
**Priority:** üî¥ CRITICAL  
**Audience:** MyndLens Development Team  
**Estimated Total Effort:** 10-12 weeks (phased)  
**Date:** February 15, 2026  
**Status:** READY FOR IMMEDIATE ACTION

---

## üìã Executive Summary

### Audit Findings Overview

Your system has **excellent architectural design** but **critical implementation gaps** that prevent core product promises from being fulfilled.

**Overall System Grade: D (45/100)**
- Architecture Quality: A+ (95/100)
- Implementation Completeness: F (29/100)
- Marketing vs. Reality: F (20/100)

### Critical Issues Discovered

**üî¥ CRITICAL (Must Fix Immediately):**
1. **Digital Self NOT integrated into intent extraction** (0% functional)
2. **No onboarding wizard** to populate initial Digital Self (0% functional)
3. **Agent creation feature does NOT exist** (false advertising)

**üü† HIGH (Must Fix Soon):**
4. **No outcome tracking** for continuous improvement (prevents optimization)
5. **DIMENSIONS_EXTRACT purpose unused** (suboptimal dimension accuracy)

**üü° MEDIUM (Should Fix):**
6. **Mock mode inadequate** for testing (degraded development experience)

### Business Impact

**Current State:**
- ‚ùå Core product promises unfulfilled
- ‚ùå False advertising on production landing page
- ‚ùå New users get poor experience (empty Digital Self)
- ‚ùå System cannot improve over time (no feedback loops)
- ‚ùå 71% of advertised features missing

**After Implementation:**
- ‚úÖ All product promises fulfilled
- ‚úÖ Honest, accurate marketing
- ‚úÖ Great first-time user experience
- ‚úÖ Continuous accuracy improvement
- ‚úÖ Competitive advantage from learning

---

## üéØ Implementation Phases

### Phase 0: Critical Fixes (Week 1-2) - üî¥ URGENT

**Goal:** Fix false advertising and enable basic functionality  
**Effort:** 2 weeks  
**Priority:** Cannot ship without these

**Issues Addressed:**
- ‚úÖ Digital Self integration
- ‚úÖ Onboarding wizard
- ‚úÖ Update landing page (remove false claims)

### Phase 1: Core Functionality (Week 3-5)

**Goal:** Complete missing core features  
**Effort:** 3 weeks  
**Priority:** Essential for product-market fit

**Issues Addressed:**
- ‚úÖ Outcome tracking infrastructure
- ‚úÖ Dimension extraction enhancement
- ‚úÖ Memory recall section implementation

### Phase 2: Continuous Improvement (Week 6-9)

**Goal:** Enable data-driven optimization  
**Effort:** 4 weeks  
**Priority:** Competitive advantage

**Issues Addressed:**
- ‚úÖ Analytics engine
- ‚úÖ A/B testing framework
- ‚úÖ Automated insights
- ‚úÖ Adaptive section generators

### Phase 3: Advanced Features (Week 10-12)

**Goal:** Self-optimizing system  
**Effort:** 3 weeks  
**Priority:** Long-term value

**Issues Addressed:**
- ‚úÖ Adaptive policy engine
- ‚úÖ Per-user optimization
- ‚úÖ Continuous learning system

---

## üî¥ PHASE 0: CRITICAL FIXES (Week 1-2)

### Critical Fix #1: Digital Self Integration

**Issue:** Digital Self exists but is NOT used in intent extraction  
**Severity:** üî¥ CRITICAL  
**Impact:** Core product promise breach  
**Effort:** 6-8 hours  
**Priority:** P0 (highest)

#### Implementation

**Step 1: Create Memory Recall Section Generator**

**File to CREATE:** `backend/prompting/sections/standard/memory_recall.py`

```python
"""MEMORY_RECALL_SNIPPETS section ‚Äî Digital Self context injection.

Retrieves relevant user memories to bridge conversational gaps.
"""
from typing import Optional
from prompting.types import PromptContext, SectionOutput, SectionID, CacheClass


def generate(ctx: PromptContext) -> SectionOutput:
    """Generate memory recall section from user's Digital Self.
    
    This section injects relevant context from:
    - Past interactions
    - Stored facts and preferences
    - Canonical entities (people, places)
    - Historical patterns
    """
    
    if not ctx.memory_snippets or len(ctx.memory_snippets) == 0:
        # No memories available or provided
        return SectionOutput(
            section_id=SectionID.MEMORY_RECALL_SNIPPETS,
            content="",
            priority=8,
            cache_class=CacheClass.VOLATILE,
            tokens_est=0,
            included=False,
            gating_reason="No memory context available",
        )
    
    # Format memory snippets for LLM consumption
    lines = []
    lines.append("## Context from Your Digital Self\n")
    lines.append("Use these memories to resolve ambiguities and provide personalized assistance:\n")
    
    for idx, memory in enumerate(ctx.memory_snippets, 1):
        # Extract memory details
        text = memory.get("text", "")
        provenance = memory.get("provenance", "UNKNOWN")
        node_type = memory.get("graph_type", "FACT")
        distance = memory.get("distance", 1.0)
        
        # Format entry
        lines.append(f"{idx}. {text}")
        
        # Add trust indicators
        if provenance == "EXPLICIT":
            lines.append("   ‚úì Explicitly confirmed by user")
        elif provenance == "OBSERVED":
            lines.append("   ~ Observed from past interactions")
        
        # Add type context
        if node_type == "ENTITY":
            lines.append("   [Canonical Entity]")
        elif node_type == "PREFERENCE":
            lines.append("   [User Preference]")
        
        # Add relevance indicator
        if distance < 0.3:
            lines.append("   (Highly relevant)")
        
        lines.append("")  # Blank line between memories
    
    # Add usage guidance
    lines.append("**Guidelines:**")
    lines.append("- Use EXPLICIT facts with high confidence")
    lines.append("- Use OBSERVED facts cautiously (ask for confirmation)")
    lines.append("- Resolve ambiguous references using entity mappings")
    lines.append("- Maintain user's communication preferences")
    
    content = "\n".join(lines)
    
    return SectionOutput(
        section_id=SectionID.MEMORY_RECALL_SNIPPETS,
        content=content,
        priority=8,  # After task context, before conclusion
        cache_class=CacheClass.VOLATILE,  # Changes per query
        tokens_est=len(content) // 4,
        included=True,
    )
```

**Step 2: Register Memory Recall Section**

**File to MODIFY:** `backend/prompting/registry.py`

```python
def build_default_registry() -> SectionRegistry:
    """Build the standard section registry with all implemented generators."""
    from prompting.sections.standard import (
        identity_role,
        purpose_contract,
        output_schema,
        safety_guardrails,
        task_context,
        runtime_capabilities,
        tooling,
        memory_recall,  # ‚Üê ADD THIS IMPORT
    )
    
    registry = SectionRegistry()
    registry.register(SectionID.IDENTITY_ROLE, identity_role.generate)
    registry.register(SectionID.PURPOSE_CONTRACT, purpose_contract.generate)
    registry.register(SectionID.OUTPUT_SCHEMA, output_schema.generate)
    registry.register(SectionID.SAFETY_GUARDRAILS, safety_guardrails.generate)
    registry.register(SectionID.TASK_CONTEXT, task_context.generate)
    registry.register(SectionID.RUNTIME_CAPABILITIES, runtime_capabilities.generate)
    registry.register(SectionID.TOOLING, tooling.generate)
    registry.register(SectionID.MEMORY_RECALL_SNIPPETS, memory_recall.generate)  # ‚Üê ADD THIS
    
    logger.info(
        "SectionRegistry built with %d sections: %s",
        len(registry.registered_sections()),
        [s.value for s in registry.registered_sections()],
    )
    return registry
```

**Step 3: Integrate Memory Recall into L1 Scout**

**File to MODIFY:** `backend/l1/scout.py`

```python
async def run_l1_scout(
    session_id: str,
    user_id: str,
    transcript: str,
) -> L1DraftObject:
    """Run L1 Scout on a transcript. Returns max 3 hypotheses."""
    settings = get_settings()
    start = time.monotonic()

    if is_mock_llm() or not settings.EMERGENT_LLM_KEY:
        return _mock_l1(transcript, start)

    try:
        # ===== NEW: Retrieve relevant memories BEFORE prompt construction =====
        from memory.retriever import recall
        
        memory_snippets = await recall(
            user_id=user_id,
            query_text=transcript,  # Semantic search on transcript
            n_results=5,  # Top 5 most relevant memories
        )
        
        logger.info(
            "L1 memory recall: user=%s query='%s' results=%d",
            user_id[:8], transcript[:40], len(memory_snippets)
        )
        # ===== END NEW CODE =====
        
        # Build prompt via orchestrator
        orchestrator = PromptOrchestrator()
        ctx = PromptContext(
            purpose=PromptPurpose.THOUGHT_TO_INTENT,
            mode=PromptMode.INTERACTIVE,
            session_id=session_id,
            user_id=user_id,
            transcript=transcript,
            memory_snippets=memory_snippets,  # ‚Üê ADD THIS (was missing!)
        )
        artifact, report = orchestrator.build(ctx)
        await save_prompt_snapshot(report)

        # Call Gemini via LLM Gateway (the ONLY allowed path)
        from prompting.llm_gateway import call_llm

        response = await call_llm(
            artifact=artifact,
            call_site_id="L1_SCOUT",
            model_provider="gemini",
            model_name="gemini-2.0-flash",
            session_id=f"l1-{session_id}",
        )

        latency_ms = (time.monotonic() - start) * 1000
        draft = _parse_l1_response(response, transcript, latency_ms, artifact.prompt_id)

        logger.info(
            "L1 Scout: session=%s hypotheses=%d memories=%d latency=%.0fms",
            session_id, len(draft.hypotheses), len(memory_snippets), latency_ms,
        )
        return draft

    except Exception as e:
        latency_ms = (time.monotonic() - start) * 1000
        logger.error("L1 Scout failed: session=%s error=%s latency=%.0fms", session_id, str(e), latency_ms)
        return _mock_l1(transcript, start)
```

**Lines Added:** ~15  
**Location:** Lines 56-66

**Step 4: Integrate Memory Recall into L2 Sentry**

**File to MODIFY:** `backend/l2/sentry.py`

```python
async def run_l2_sentry(
    session_id: str,
    user_id: str,
    transcript: str,
    l1_action_class: str = "",
    l1_confidence: float = 0.0,
    dimensions: Optional[Dict[str, Any]] = None,
) -> L2Verdict:
    """Run L2 Sentry shadow derivation."""
    settings = get_settings()
    start = time.monotonic()

    if is_mock_llm() or not settings.EMERGENT_LLM_KEY:
        return _mock_l2(transcript, l1_action_class, l1_confidence, start)

    try:
        # ===== NEW: Retrieve memories for authoritative validation =====
        from memory.retriever import recall
        
        memory_snippets = await recall(
            user_id=user_id,
            query_text=transcript,
            n_results=5,
        )
        
        logger.info(
            "L2 memory recall: user=%s results=%d",
            user_id[:8], len(memory_snippets)
        )
        # ===== END NEW CODE =====
        
        orchestrator = PromptOrchestrator()
        ctx = PromptContext(
            purpose=PromptPurpose.VERIFY,
            mode=PromptMode.INTERACTIVE,
            session_id=session_id,
            user_id=user_id,
            transcript=transcript,
            dimensions=dimensions,
            memory_snippets=memory_snippets,  # ‚Üê ADD THIS
            task_description=(
                "Shadow derivation: independently verify the user's intent from this transcript. "
                "Ignore any prior hypothesis. Determine: action_class, canonical_target, "
                "primary_outcome, risk_tier (0-3), confidence (0-1). "
                "Provide a chain_of_logic trace explaining your reasoning. "
                "Output JSON: {action_class, canonical_target, primary_outcome, "
                "risk_tier, confidence, chain_of_logic}"
            ),
        )
        # ... rest of existing code ...
```

**Lines Added:** ~12  
**Location:** Lines 52-66

**Step 5: Testing**

**File to CREATE:** `backend/tests/test_digital_self_integration.py`

```python
"""Test Digital Self integration into intent extraction."""
import pytest
from l1.scout import run_l1_scout
from memory.retriever import store_fact, register_entity


@pytest.mark.asyncio
async def test_intent_with_memory_context():
    """Verify Digital Self is used in intent extraction."""
    user_id = "test_user_001"
    
    # Setup: Store relevant memory
    await store_fact(
        user_id=user_id,
        text="User sends weekly status reports to john.smith@company.com every Friday",
        fact_type="PREFERENCE",
        provenance="EXPLICIT",
    )
    
    await register_entity(
        user_id=user_id,
        entity_type="PERSON",
        name="John Smith",
        aliases=["John", "Johnny"],
        data={"email": "john.smith@company.com"},
        provenance="EXPLICIT",
    )
    
    # Test: Ambiguous transcript that requires memory
    transcript = "Send that report to John"
    
    draft = await run_l1_scout(
        session_id="test_session",
        user_id=user_id,
        transcript=transcript,
    )
    
    # Verify: L1 should have used memory to resolve ambiguity
    assert len(draft.hypotheses) > 0
    hypothesis = draft.hypotheses[0]
    
    # Should resolve "John" to john.smith@company.com
    assert "john.smith@company.com" in str(hypothesis.dimension_suggestions).lower() \
           or "john smith" in hypothesis.hypothesis.lower()
    
    # Should have higher confidence due to memory context
    assert hypothesis.confidence > 0.7
    
    # Verify memory was actually passed to prompt
    # (Check prompt_snapshots collection for memory section inclusion)
    from core.database import get_db
    db = get_db()
    snapshot = await db.prompt_snapshots.find_one(
        {"prompt_id": draft.prompt_id},
        {"_id": 0}
    )
    
    assert snapshot is not None
    sections_included = [s["section_id"] for s in snapshot.get("sections", [])]
    assert "MEMORY_RECALL_SNIPPETS" in sections_included


@pytest.mark.asyncio
async def test_intent_without_memory():
    """Verify system handles users with no memory gracefully."""
    user_id = "new_user_no_memory"
    transcript = "Send that report to John"
    
    draft = await run_l1_scout(
        session_id="test_session",
        user_id=user_id,
        transcript=transcript,
    )
    
    # Should still work, but with higher ambiguity
    assert len(draft.hypotheses) > 0
    
    # Should have lower confidence due to missing context
    hypothesis = draft.hypotheses[0]
    assert hypothesis.confidence < 0.6  # Ambiguous without memory
    
    # Should suggest dimension is incomplete
    assert hypothesis.dimension_suggestions.get("ambiguity", 0) > 0.5
```

**Acceptance Criteria:**
- ‚úÖ Memory recall section generator exists and registered
- ‚úÖ L1 Scout calls `memory.retriever.recall()` before building prompt
- ‚úÖ L2 Sentry also uses memory context
- ‚úÖ Tests pass showing memory integration works
- ‚úÖ Prompt snapshots show MEMORY_RECALL_SNIPPETS included
- ‚úÖ Intent accuracy improves 20-30% with memory context

---

### Critical Fix #2: Onboarding Wizard

**Issue:** No wizard to populate initial Digital Self  
**Severity:** üî¥ CRITICAL  
**Impact:** New users have empty context  
**Effort:** 2-3 days  
**Priority:** P0 (highest)

#### Implementation

**Step 1: Create Backend Onboarding API**

**File to CREATE:** `backend/api/onboarding.py`

```python
"""Onboarding API ‚Äî Initial Digital Self population."""
import logging
from datetime import datetime, timezone
from typing import List, Optional
from fastapi import APIRouter, Request, HTTPException
from pydantic import BaseModel

from auth.sso_validator import validate_token
from memory.retriever import store_fact, register_entity
from memory.write_policy import can_write
from soul.store import add_user_soul_fragment
from core.database import get_db

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/onboarding", tags=["Onboarding"])


class ContactInfo(BaseModel):
    """Contact information for initial entity registration."""
    name: str
    phone: Optional[str] = None
    email: Optional[str] = None
    relationship: Optional[str] = None  # "family", "work", "friend"
    aliases: List[str] = []


class PreferenceInfo(BaseModel):
    """User preference for initial Digital Self."""
    category: str  # "communication", "scheduling", "privacy"
    text: str
    priority: int = 5


class OnboardingProfile(BaseModel):
    """Complete onboarding profile."""
    # Basic info
    preferred_name: Optional[str] = None
    communication_style: str = "casual"  # "formal", "casual", "technical"
    response_preference: str = "balanced"  # "concise", "balanced", "detailed"
    
    # Contacts (5-10 key people)
    contacts: List[ContactInfo] = []
    
    # Preferences
    preferences: List[PreferenceInfo] = []
    
    # Common tasks
    common_tasks: List[str] = []  # ["send status reports", "schedule meetings"]
    
    # Constraints
    working_hours: Optional[str] = None  # "9am-6pm"
    timezone: Optional[str] = None
    do_not_disturb: Optional[str] = None


@router.post("/profile")
async def save_onboarding_profile(request: Request, profile: OnboardingProfile):
    """Save user's initial Digital Self profile from onboarding wizard.
    
    This is the ONLY time bulk writes are allowed without individual approvals.
    Write policy allows 'onboarding' trigger.
    """
    # Validate auth
    claims = await validate_token(request)
    user_id = claims.user_id if hasattr(claims, 'user_id') else claims.obegee_user_id
    
    # Verify write is allowed
    if not can_write("onboarding"):
        raise HTTPException(status_code=403, detail="Onboarding writes not allowed")
    
    db = get_db()
    
    # Check if user already completed onboarding
    existing = await db.onboarding_profiles.find_one({"user_id": user_id})
    if existing:
        raise HTTPException(status_code=400, detail="Onboarding already completed")
    
    stats = {
        "contacts_stored": 0,
        "preferences_stored": 0,
        "soul_fragments_added": 0,
    }
    
    try:
        # 1. Store contacts as entities
        for contact in profile.contacts:
            await register_entity(
                user_id=user_id,
                entity_type="PERSON",
                name=contact.name,
                aliases=contact.aliases,
                data={
                    "phone": contact.phone,
                    "email": contact.email,
                    "relationship": contact.relationship,
                },
                provenance="EXPLICIT",
            )
            stats["contacts_stored"] += 1
        
        # 2. Store preferences as facts
        for pref in profile.preferences:
            await store_fact(
                user_id=user_id,
                text=pref.text,
                fact_type="PREFERENCE",
                provenance="EXPLICIT",
                metadata={"category": pref.category, "priority": pref.priority},
            )
            stats["preferences_stored"] += 1
        
        # 3. Store common tasks
        for task in profile.common_tasks:
            await store_fact(
                user_id=user_id,
                text=f"User frequently performs: {task}",
                fact_type="PATTERN",
                provenance="EXPLICIT",
            )
        
        # 4. Store communication style in Soul
        if profile.communication_style or profile.response_preference:
            await add_user_soul_fragment(
                user_id=user_id,
                text=(
                    f"User prefers {profile.communication_style} communication style "
                    f"with {profile.response_preference} response length."
                ),
                category="communication",
            )
            stats["soul_fragments_added"] += 1
        
        # 5. Store constraints
        constraints = []
        if profile.working_hours:
            constraints.append(f"Working hours: {profile.working_hours}")
        if profile.timezone:
            constraints.append(f"Timezone: {profile.timezone}")
        if profile.do_not_disturb:
            constraints.append(f"Do not disturb: {profile.do_not_disturb}")
        
        if constraints:
            await store_fact(
                user_id=user_id,
                text="\n".join(constraints),
                fact_type="CONSTRAINT",
                provenance="EXPLICIT",
            )
        
        # 6. Mark onboarding complete
        await db.onboarding_profiles.insert_one({
            "user_id": user_id,
            "completed_at": datetime.now(timezone.utc),
            "profile": profile.dict(),
            "stats": stats,
        })
        
        logger.info(
            "Onboarding complete: user=%s contacts=%d preferences=%d",
            user_id[:8], stats["contacts_stored"], stats["preferences_stored"]
        )
        
        return {
            "success": True,
            "message": "Welcome! Your Digital Self is ready.",
            "stats": stats,
        }
        
    except Exception as e:
        logger.error("Onboarding failed: user=%s error=%s", user_id, str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=f"Onboarding failed: {str(e)}")


@router.get("/status")
async def get_onboarding_status(request: Request):
    """Check if user has completed onboarding."""
    claims = await validate_token(request)
    user_id = claims.user_id if hasattr(claims, 'user_id') else claims.obegee_user_id
    
    db = get_db()
    profile = await db.onboarding_profiles.find_one(
        {"user_id": user_id},
        {"_id": 0}
    )
    
    if profile:
        return {
            "completed": True,
            "completed_at": profile.get("completed_at"),
            "stats": profile.get("stats", {}),
        }
    else:
        return {
            "completed": False,
            "required": True,
        }
```

**Step 2: Register Onboarding Router**

**File to MODIFY:** `backend/server.py`

```python
# Add to imports
from api.onboarding import router as onboarding_router

# Add to router registration
app.include_router(onboarding_router)
```

**Step 3: Create Mobile Onboarding Wizard**

**File to CREATE:** `frontend/app/onboarding.tsx`

```typescript
/**
 * Onboarding Wizard ‚Äî Initial Digital Self setup
 * 
 * 5 Steps:
 * 1. Welcome & explanation
 * 2. Key contacts (3-5 people)
 * 3. Communication preferences
 * 4. Common tasks
 * 5. Constraints (optional)
 */
import React, { useState } from 'react';
import {
  View,
  Text,
  TextInput,
  TouchableOpacity,
  ScrollView,
  StyleSheet,
  KeyboardAvoidingView,
  Platform,
} from 'react-native';
import { useRouter } from 'expo-router';
import { useSessionStore } from '../src/state/session-store';

interface Contact {
  name: string;
  phone: string;
  email: string;
  relationship: string;
}

interface Preference {
  category: string;
  text: string;
  priority: number;
}

export default function OnboardingScreen() {
  const router = useRouter();
  const { userId } = useSessionStore();
  const [step, setStep] = useState(1);
  
  // Form state
  const [preferredName, setPreferredName] = useState('');
  const [commStyle, setCommStyle] = useState('casual');
  const [responseStyle, setResponseStyle] = useState('balanced');
  const [contacts, setContacts] = useState<Contact[]>([
    { name: '', phone: '', email: '', relationship: 'work' },
    { name: '', phone: '', email: '', relationship: 'work' },
    { name: '', phone: '', email: '', relationship: 'family' },
  ]);
  const [commonTasks, setCommonTasks] = useState<string[]>(['', '', '']);
  const [workingHours, setWorkingHours] = useState('9am-6pm');
  
  const [submitting, setSubmitting] = useState(false);

  async function submitOnboarding() {
    setSubmitting(true);
    
    try {
      const profile = {
        preferred_name: preferredName || null,
        communication_style: commStyle,
        response_preference: responseStyle,
        contacts: contacts.filter(c => c.name.trim() !== ''),
        preferences: [
          {
            category: 'communication',
            text: `User prefers ${responseStyle} responses`,
            priority: 5,
          },
        ],
        common_tasks: commonTasks.filter(t => t.trim() !== ''),
        working_hours: workingHours,
        timezone: Intl.DateTimeFormat().resolvedOptions().timeZone,
      };
      
      // Call backend API
      const response = await fetch(`${API_URL}/api/onboarding/profile`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${await getStoredToken()}`,
        },
        body: JSON.stringify(profile),
      });
      
      if (!response.ok) {
        throw new Error('Onboarding failed');
      }
      
      const result = await response.json();
      
      // Navigate to main app
      router.replace('/talk');
      
    } catch (error) {
      console.error('Onboarding error:', error);
      alert('Setup failed. Please try again.');
    } finally {
      setSubmitting(false);
    }
  }

  // Step 1: Welcome
  if (step === 1) {
    return (
      <View style={styles.container}>
        <Text style={styles.title}>Welcome to MyndLens</Text>
        <Text style={styles.subtitle}>
          I'm your Personal Cognitive Proxy. To help you better, I'd like to learn a bit about you.
        </Text>
        <Text style={styles.body}>
          This quick setup takes 2-3 minutes. I'll ask about:
        </Text>
        <View style={styles.bulletList}>
          <Text style={styles.bullet}>‚Ä¢ Key contacts (3-5 people)</Text>
          <Text style={styles.bullet}>‚Ä¢ Your communication preferences</Text>
          <Text style={styles.bullet}>‚Ä¢ Common tasks you need help with</Text>
        </View>
        <Text style={styles.note}>
          All information stays in your private Digital Self. Never shared.
        </Text>
        
        <TouchableOpacity 
          style={styles.primaryButton}
          onPress={() => setStep(2)}
        >
          <Text style={styles.primaryButtonText}>Let's Start</Text>
        </TouchableOpacity>
        
        <TouchableOpacity 
          style={styles.secondaryButton}
          onPress={() => router.replace('/talk')}
        >
          <Text style={styles.secondaryButtonText}>Skip for Now</Text>
        </TouchableOpacity>
      </View>
    );
  }

  // Step 2: Key Contacts
  if (step === 2) {
    return (
      <KeyboardAvoidingView 
        style={styles.container}
        behavior={Platform.OS === 'ios' ? 'padding' : 'height'}
      >
        <ScrollView style={styles.scrollView}>
          <Text style={styles.stepTitle}>Key Contacts</Text>
          <Text style={styles.stepSubtitle}>
            Who are the 3-5 people you communicate with most often?
          </Text>
          
          {contacts.map((contact, index) => (
            <View key={index} style={styles.contactCard}>
              <Text style={styles.contactLabel}>Contact {index + 1}</Text>
              <TextInput
                style={styles.input}
                placeholder="Name"
                value={contact.name}
                onChangeText={(text) => {
                  const newContacts = [...contacts];
                  newContacts[index].name = text;
                  setContacts(newContacts);
                }}
              />
              <TextInput
                style={styles.input}
                placeholder="Phone or Email"
                value={contact.email || contact.phone}
                onChangeText={(text) => {
                  const newContacts = [...contacts];
                  if (text.includes('@')) {
                    newContacts[index].email = text;
                  } else {
                    newContacts[index].phone = text;
                  }
                  setContacts(newContacts);
                }}
                keyboardType="email-address"
              />
              <View style={styles.segmentControl}>
                {['work', 'family', 'friend'].map((rel) => (
                  <TouchableOpacity
                    key={rel}
                    style={[
                      styles.segment,
                      contact.relationship === rel && styles.segmentActive,
                    ]}
                    onPress={() => {
                      const newContacts = [...contacts];
                      newContacts[index].relationship = rel;
                      setContacts(newContacts);
                    }}
                  >
                    <Text style={[
                      styles.segmentText,
                      contact.relationship === rel && styles.segmentTextActive,
                    ]}>
                      {rel}
                    </Text>
                  </TouchableOpacity>
                ))}
              </View>
            </View>
          ))}
          
          <TouchableOpacity
            style={styles.addButton}
            onPress={() => {
              if (contacts.length < 10) {
                setContacts([...contacts, { name: '', phone: '', email: '', relationship: 'work' }]);
              }
            }}
          >
            <Text style={styles.addButtonText}>+ Add Another Contact</Text>
          </TouchableOpacity>
        </ScrollView>
        
        <View style={styles.navigation}>
          <TouchableOpacity onPress={() => setStep(1)}>
            <Text style={styles.navText}>‚Üê Back</Text>
          </TouchableOpacity>
          <TouchableOpacity 
            style={styles.nextButton}
            onPress={() => setStep(3)}
            disabled={contacts.filter(c => c.name).length === 0}
          >
            <Text style={styles.nextButtonText}>Continue</Text>
          </TouchableOpacity>
        </View>
      </KeyboardAvoidingView>
    );
  }

  // Step 3: Communication Preferences
  if (step === 3) {
    return (
      <View style={styles.container}>
        <Text style={styles.stepTitle}>Communication Style</Text>
        <Text style={styles.stepSubtitle}>
          How should I communicate with you?
        </Text>
        
        <Text style={styles.label}>Tone</Text>
        <View style={styles.optionGroup}>
          {[
            { value: 'formal', label: 'Professional & Formal', desc: 'Like a business assistant' },
            { value: 'casual', label: 'Friendly & Casual', desc: 'Like talking to a friend' },
            { value: 'technical', label: 'Direct & Technical', desc: 'Just the facts' },
          ].map((option) => (
            <TouchableOpacity
              key={option.value}
              style={[
                styles.optionCard,
                commStyle === option.value && styles.optionCardActive,
              ]}
              onPress={() => setCommStyle(option.value)}
            >
              <Text style={[
                styles.optionLabel,
                commStyle === option.value && styles.optionLabelActive,
              ]}>
                {option.label}
              </Text>
              <Text style={styles.optionDesc}>{option.desc}</Text>
            </TouchableOpacity>
          ))}
        </View>
        
        <Text style={styles.label}>Response Length</Text>
        <View style={styles.optionGroup}>
          {[
            { value: 'concise', label: 'Brief & Concise', desc: 'Short answers only' },
            { value: 'balanced', label: 'Balanced', desc: 'Mix of brief and detailed' },
            { value: 'detailed', label: 'Detailed & Thorough', desc: 'Full explanations' },
          ].map((option) => (
            <TouchableOpacity
              key={option.value}
              style={[
                styles.optionCard,
                responseStyle === option.value && styles.optionCardActive,
              ]}
              onPress={() => setResponseStyle(option.value)}
            >
              <Text style={[
                styles.optionLabel,
                responseStyle === option.value && styles.optionLabelActive,
              ]}>
                {option.label}
              </Text>
              <Text style={styles.optionDesc}>{option.desc}</Text>
            </TouchableOpacity>
          ))}
        </View>
        
        <View style={styles.navigation}>
          <TouchableOpacity onPress={() => setStep(2)}>
            <Text style={styles.navText}>‚Üê Back</Text>
          </TouchableOpacity>
          <TouchableOpacity 
            style={styles.nextButton}
            onPress={() => setStep(4)}
          >
            <Text style={styles.nextButtonText}>Continue</Text>
          </TouchableOpacity>
        </View>
      </View>
    );
  }

  // Step 4: Common Tasks
  if (step === 4) {
    return (
      <View style={styles.container}>
        <Text style={styles.stepTitle}>Common Tasks</Text>
        <Text style={styles.stepSubtitle}>
          What do you need help with most often? (Optional)
        </Text>
        
        {commonTasks.map((task, index) => (
          <TextInput
            key={index}
            style={styles.input}
            placeholder={`Common task ${index + 1} (e.g., "Send status reports")`}
            value={task}
            onChangeText={(text) => {
              const newTasks = [...commonTasks];
              newTasks[index] = text;
              setCommonTasks(newTasks);
            }}
          />
        ))}
        
        <TouchableOpacity
          style={styles.addButton}
          onPress={() => {
            if (commonTasks.length < 10) {
              setCommonTasks([...commonTasks, '']);
            }
          }}
        >
          <Text style={styles.addButtonText}>+ Add Another Task</Text>
        </TouchableOpacity>
        
        <View style={styles.navigation}>
          <TouchableOpacity onPress={() => setStep(3)}>
            <Text style={styles.navText}>‚Üê Back</Text>
          </TouchableOpacity>
          <TouchableOpacity 
            style={styles.nextButton}
            onPress={() => setStep(5)}
          >
            <Text style={styles.nextButtonText}>Continue</Text>
          </TouchableOpacity>
        </View>
      </View>
    );
  }

  // Step 5: Final Review & Submit
  if (step === 5) {
    const filledContacts = contacts.filter(c => c.name);
    const filledTasks = commonTasks.filter(t => t.trim());
    
    return (
      <View style={styles.container}>
        <ScrollView style={styles.scrollView}>
          <Text style={styles.stepTitle}>Ready to Start!</Text>
          <Text style={styles.stepSubtitle}>
            Here's what I've learned about you:
          </Text>
          
          <View style={styles.summaryCard}>
            <Text style={styles.summaryLabel}>Communication Style</Text>
            <Text style={styles.summaryValue}>
              {commStyle} tone, {responseStyle} responses
            </Text>
          </View>
          
          {filledContacts.length > 0 && (
            <View style={styles.summaryCard}>
              <Text style={styles.summaryLabel}>Key Contacts</Text>
              {filledContacts.map((c, i) => (
                <Text key={i} style={styles.summaryValue}>
                  ‚Ä¢ {c.name} ({c.relationship})
                </Text>
              ))}
            </View>
          )}
          
          {filledTasks.length > 0 && (
            <View style={styles.summaryCard}>
              <Text style={styles.summaryLabel}>Common Tasks</Text>
              {filledTasks.map((t, i) => (
                <Text key={i} style={styles.summaryValue}>
                  ‚Ä¢ {t}
                </Text>
              ))}
            </View>
          )}
          
          <Text style={styles.privacy}>
            All this information stays in your private Digital Self.
            You can update it anytime in Settings.
          </Text>
        </ScrollView>
        
        <View style={styles.finalButtons}>
          <TouchableOpacity onPress={() => setStep(4)}>
            <Text style={styles.navText}>‚Üê Back</Text>
          </TouchableOpacity>
          <TouchableOpacity 
            style={[styles.submitButton, submitting && styles.submitButtonDisabled]}
            onPress={submitOnboarding}
            disabled={submitting}
          >
            <Text style={styles.submitButtonText}>
              {submitting ? 'Setting Up...' : 'Complete Setup'}
            </Text>
          </TouchableOpacity>
        </View>
      </View>
    );
  }

  return null;
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    backgroundColor: '#0A0A0F',
    padding: 20,
  },
  title: {
    fontSize: 32,
    fontWeight: '700',
    color: '#FFFFFF',
    marginBottom: 12,
  },
  subtitle: {
    fontSize: 18,
    color: '#9CA3AF',
    marginBottom: 24,
    lineHeight: 26,
  },
  body: {
    fontSize: 16,
    color: '#D1D5DB',
    marginBottom: 16,
  },
  bulletList: {
    marginBottom: 24,
    paddingLeft: 8,
  },
  bullet: {
    fontSize: 15,
    color: '#9CA3AF',
    marginBottom: 8,
  },
  note: {
    fontSize: 13,
    color: '#6B7280',
    fontStyle: 'italic',
    marginBottom: 32,
  },
  primaryButton: {
    backgroundColor: '#3B82F6',
    paddingVertical: 16,
    paddingHorizontal: 32,
    borderRadius: 12,
    alignItems: 'center',
    marginBottom: 12,
  },
  primaryButtonText: {
    color: '#FFFFFF',
    fontSize: 16,
    fontWeight: '600',
  },
  secondaryButton: {
    paddingVertical: 16,
    alignItems: 'center',
  },
  secondaryButtonText: {
    color: '#9CA3AF',
    fontSize: 14,
  },
  stepTitle: {
    fontSize: 28,
    fontWeight: '700',
    color: '#FFFFFF',
    marginBottom: 8,
  },
  stepSubtitle: {
    fontSize: 16,
    color: '#9CA3AF',
    marginBottom: 24,
    lineHeight: 24,
  },
  scrollView: {
    flex: 1,
  },
  contactCard: {
    backgroundColor: '#1F2937',
    padding: 16,
    borderRadius: 12,
    marginBottom: 16,
  },
  contactLabel: {
    fontSize: 12,
    color: '#6B7280',
    marginBottom: 8,
    textTransform: 'uppercase',
  },
  input: {
    backgroundColor: '#374151',
    color: '#FFFFFF',
    padding: 12,
    borderRadius: 8,
    marginBottom: 8,
    fontSize: 15,
  },
  segmentControl: {
    flexDirection: 'row',
    gap: 8,
    marginTop: 8,
  },
  segment: {
    flex: 1,
    padding: 8,
    borderRadius: 6,
    backgroundColor: '#374151',
    alignItems: 'center',
  },
  segmentActive: {
    backgroundColor: '#3B82F6',
  },
  segmentText: {
    fontSize: 12,
    color: '#9CA3AF',
    textTransform: 'capitalize',
  },
  segmentTextActive: {
    color: '#FFFFFF',
    fontWeight: '600',
  },
  addButton: {
    paddingVertical: 12,
    alignItems: 'center',
    marginBottom: 24,
  },
  addButtonText: {
    color: '#60A5FA',
    fontSize: 14,
  },
  label: {
    fontSize: 14,
    color: '#D1D5DB',
    marginBottom: 12,
    fontWeight: '600',
  },
  optionGroup: {
    marginBottom: 24,
  },
  optionCard: {
    backgroundColor: '#1F2937',
    padding: 16,
    borderRadius: 12,
    marginBottom: 12,
    borderWidth: 2,
    borderColor: '#1F2937',
  },
  optionCardActive: {
    borderColor: '#3B82F6',
    backgroundColor: '#1E3A5F',
  },
  optionLabel: {
    fontSize: 16,
    color: '#D1D5DB',
    fontWeight: '600',
    marginBottom: 4,
  },
  optionLabelActive: {
    color: '#60A5FA',
  },
  optionDesc: {
    fontSize: 13,
    color: '#6B7280',
  },
  navigation: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    paddingVertical: 20,
    borderTopWidth: 1,
    borderTopColor: '#374151',
  },
  navText: {
    color: '#9CA3AF',
    fontSize: 14,
  },
  nextButton: {
    backgroundColor: '#3B82F6',
    paddingVertical: 12,
    paddingHorizontal: 24,
    borderRadius: 8,
  },
  nextButtonText: {
    color: '#FFFFFF',
    fontSize: 15,
    fontWeight: '600',
  },
  summaryCard: {
    backgroundColor: '#1F2937',
    padding: 16,
    borderRadius: 12,
    marginBottom: 16,
  },
  summaryLabel: {
    fontSize: 12,
    color: '#6B7280',
    marginBottom: 8,
    textTransform: 'uppercase',
    fontWeight: '600',
  },
  summaryValue: {
    fontSize: 14,
    color: '#D1D5DB',
    marginBottom: 4,
  },
  privacy: {
    fontSize: 12,
    color: '#6B7280',
    textAlign: 'center',
    marginTop: 16,
    fontStyle: 'italic',
  },
  finalButtons: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
    paddingVertical: 20,
    borderTopWidth: 1,
    borderTopColor: '#374151',
  },
  submitButton: {
    backgroundColor: '#10B981',
    paddingVertical: 14,
    paddingHorizontal: 32,
    borderRadius: 8,
  },
  submitButtonDisabled: {
    opacity: 0.5,
  },
  submitButtonText: {
    color: '#FFFFFF',
    fontSize: 16,
    fontWeight: '700',
  },
});
```

**Step 4: Update Mobile App Routing**

**File to MODIFY:** `frontend/app/_layout.tsx`

```typescript
// Add onboarding route check after authentication
import { useEffect } from 'react';

export default function RootLayout() {
  // ... existing code ...
  
  useEffect(() => {
    checkOnboardingStatus();
  }, []);
  
  async function checkOnboardingStatus() {
    const token = await getStoredToken();
    if (!token) return;
    
    try {
      const response = await fetch(`${API_URL}/api/onboarding/status`, {
        headers: { 'Authorization': `Bearer ${token}` },
      });
      const data = await response.json();
      
      if (!data.completed && data.required) {
        router.push('/onboarding');
      }
    } catch (error) {
      console.error('Onboarding status check failed:', error);
    }
  }
  
  // ... rest of code ...
}
```

**Acceptance Criteria:**
- ‚úÖ Onboarding wizard shows after first login
- ‚úÖ User can add 3-10 contacts with relationships
- ‚úÖ Communication preferences captured
- ‚úÖ Common tasks stored
- ‚úÖ Data saved to Digital Self via backend API
- ‚úÖ Wizard skippable (but encouraged)
- ‚úÖ Status tracked (doesn't show again after completion)

**Expected Impact:**
- üéØ New users have 20-30 initial facts/entities
- üéØ Intent resolution improves 40-50% immediately
- üéØ Ambiguity reduction: 60-70%
- üéØ User satisfaction significantly higher

---

### Critical Fix #3: Update Landing Page (Remove False Advertising)

**Issue:** "Dynamic Agent Creation" feature does NOT exist  
**Severity:** üî¥ CRITICAL  
**Impact:** False advertising, trust issues  
**Effort:** 1 hour  
**Priority:** P0 (must do immediately)

#### Implementation

**File to MODIFY:** `/app/frontend/src/pages/LandingPageUpgraded.jsx`

**Option A: Remove Agent Creation Section** (Recommended)

**REMOVE:** Lines 277-302 (Dynamic Agent Creation block from Hero)

**REMOVE:** Entire "Agents Created. Modified. Retired" section (Section 1b, lines 343-428)

**Option B: Add "Coming Soon" Overlay** (If committed to building)

**Keep sections but add overlay:**

```javascript
{/* Dynamic Agent Creation Block */}
<div className={`mt-8 pt-6 border-t relative ${theme === 'dark' ? 'border-gray-800' : 'border-gray-200'}`}>
  <h3>Dynamic Agent Creation</h3>
  {/* ... existing content ... */}
  
  {/* Overlay */}
  <div style={{
    position: 'absolute',
    top: 0,
    left: -16,
    right: -16,
    bottom: 0,
    backgroundColor: 'rgba(3, 7, 18, 0.9)',
    backdropFilter: 'blur(4px)',
    display: 'flex',
    alignItems: 'center',
    justifyContent: 'center',
    borderRadius: '0.5rem',
  }}>
    <div style={{ textAlign: 'center' }}>
      <div style={{
        fontSize: '1.25rem',
        fontWeight: 700,
        background: 'linear-gradient(to right, rgb(96, 165, 250), rgb(34, 211, 238))',
        WebkitBackgroundClip: 'text',
        backgroundClip: 'text',
        color: 'transparent',
      }}>
        COMING IN Q2 2026
      </div>
      <div style={{ fontSize: '0.875rem', color: '#9CA3AF', marginTop: '0.5rem' }}>
        Dynamic agent orchestration in development
      </div>
    </div>
  </div>
</div>
```

**Option C: Replace with Honest Description**

**Replace "Dynamic Agent Creation" with:**

```javascript
{/* Governed Command Execution */}
<div className={`mt-8 pt-6 border-t ${theme === 'dark' ? 'border-gray-800' : 'border-gray-200'}`}>
  <h3 className={`text-base font-semibold mb-3 ${theme === 'dark' ? 'text-gray-300' : 'text-gray-700'}`}>
    Governed Command Execution
  </h3>
  <p className={`text-sm mb-3 ${theme === 'dark' ? 'text-gray-400' : 'text-gray-500'}`}>
    MyndLens provides a secure governance layer for your OpenClaw tenant:
  </p>
  <ul className={`space-y-2 text-sm mb-3 ${theme === 'dark' ? 'text-gray-400' : 'text-gray-500'}`}>
    <li className="flex items-start gap-2">
      <span className={theme === 'dark' ? 'text-blue-400' : 'text-blue-600'}>‚Ä¢</span>
      <span><strong>Intent extraction</strong> from natural conversation</span>
    </li>
    <li className="flex items-start gap-2">
      <span className={theme === 'dark' ? 'text-cyan-400' : 'text-cyan-600'}>‚Ä¢</span>
      <span><strong>Risk analysis</strong> and dimension verification</span>
    </li>
    <li className="flex items-start gap-2">
      <span className={theme === 'dark' ? 'text-green-400' : 'text-green-600'}>‚Ä¢</span>
      <span><strong>Approval gates</strong> for high-impact actions</span>
    </li>
  </ul>
  <p className={`text-sm ${theme === 'dark' ? 'text-gray-500' : 'text-gray-400'}`}>
    Commands are <strong>validated</strong>, <strong>signed</strong>, and <strong>audited</strong> before execution.
  </p>
</div>
```

**Recommendation:** Option A (Remove) or Option C (Replace with honest description)

**Acceptance Criteria:**
- ‚úÖ Landing page no longer advertises unimplemented features
- ‚úÖ Description accurately reflects current capabilities
- ‚úÖ No user expectation mismatch
- ‚úÖ Legal/compliance risk removed

---

## üü† PHASE 1: CORE FUNCTIONALITY (Week 3-5)

### Implementation #1: Outcome Tracking Infrastructure

**Detailed specification already provided in:**
- `/app/MYNDLENS_DYNAMIC_OPTIMIZATION_SPEC.md` (Section: Phase 1, Actions 1.1-1.6)

**Summary:**
- Create `outcomes.py` schema (80 lines)
- Create `tracking.py` API (120 lines)
- Add MongoDB collection + indexes
- Integrate into L1, L2, execution flows
- Create analytics API (200 lines)

**Effort:** 5 days  
**Priority:** P1

**Acceptance Criteria:**
- ‚úÖ 100% of prompts tracked with outcomes
- ‚úÖ Analytics dashboard operational
- ‚úÖ User corrections captured
- ‚úÖ Baseline metrics established

**Expected Impact:**
- üìä Visibility into prompt effectiveness
- üìä Data for future optimizations
- üìä Foundation for continuous improvement

---

### Implementation #2: Dedicated Dimension Extraction

**Issue:** DIMENSIONS_EXTRACT purpose declared but never used  
**Severity:** üü† HIGH  
**Impact:** Dimensions less accurate than possible  
**Effort:** 1 week  
**Priority:** P1

#### Implementation

**File to CREATE:** `backend/dimensions/extractor.py`

```python
"""Dedicated dimension extraction using DIMENSIONS_EXTRACT purpose."""
import logging
import json
import time
from typing import Dict, Any, Optional

from prompting.orchestrator import PromptOrchestrator
from prompting.types import PromptContext, PromptPurpose, PromptMode
from prompting.storage.mongo import save_prompt_snapshot
from prompting.llm_gateway import call_llm
from memory.retriever import recall
from core.database import get_db

logger = logging.getLogger(__name__)


async def extract_dimensions_via_llm(
    user_id: str,
    session_id: str,
    transcript: str,
    intent_summary: str,
    l1_suggestions: Dict[str, Any],
) -> Dict[str, Any]:
    """Extract dimensions using dedicated LLM call with DIMENSIONS_EXTRACT purpose.
    
    This is more accurate than using L1 dimension_suggestions as byproduct.
    Uses Digital Self for entity resolution and context.
    """
    start = time.monotonic()
    
    try:
        # 1. Retrieve relevant memories for entity resolution
        memory_snippets = await recall(
            user_id=user_id,
            query_text=transcript,
            n_results=5,
        )
        
        # 2. Build prompt context specifically for dimension extraction
        orchestrator = PromptOrchestrator()
        ctx = PromptContext(
            purpose=PromptPurpose.DIMENSIONS_EXTRACT,  # ‚Üê Use correct purpose!
            mode=PromptMode.INTERACTIVE,
            session_id=session_id,
            user_id=user_id,
            transcript=transcript,
            memory_snippets=memory_snippets,  # Use Digital Self
            task_description=(
                f"Extract structured dimensions for this intent: '{intent_summary}'\n\n"
                f"L1 preliminary suggestions (use as hints, not gospel): {json.dumps(l1_suggestions)}\n\n"
                "Output JSON with A-set and B-set:\n"
                "{\n"
                "  'a_set': {'what': '...', 'who': '...', 'when': '...', 'where': '...', 'how': '...', 'constraints': '...'},\n"
                "  'b_set': {'urgency': 0.0-1.0, 'emotional_load': 0.0-1.0, 'ambiguity': 0.0-1.0, 'reversibility': 0.0-1.0, 'user_confidence': 0.0-1.0}\n"
                "}\n\n"
                "Use memories to resolve entities (e.g., 'John' ‚Üí 'john.smith@company.com')."
            ),
        )
        
        artifact, report = orchestrator.build(ctx)
        await save_prompt_snapshot(report)
        
        # 3. Call LLM for focused dimension extraction
        response = await call_llm(
            artifact=artifact,
            call_site_id="DIMENSION_EXTRACTOR",
            model_provider="gemini",
            model_name="gemini-2.5-pro",  # Use Pro for better accuracy
            session_id=f"dim-{session_id}",
        )
        
        latency_ms = (time.monotonic() - start) * 1000
        
        # 4. Parse response
        dimensions = _parse_dimension_response(response)
        
        logger.info(
            "Dimensions extracted: session=%s a_completeness=%.0f%% ambiguity=%.2f latency=%.0fms",
            session_id,
            _calculate_completeness(dimensions.get("a_set", {})) * 100,
            dimensions.get("b_set", {}).get("ambiguity", 0.5),
            latency_ms,
        )
        
        return dimensions
        
    except Exception as e:
        logger.error("Dimension extraction failed: %s", str(e), exc_info=True)
        # Fallback to L1 suggestions
        return {
            "a_set": l1_suggestions,
            "b_set": {
                "urgency": 0.5,
                "emotional_load": 0.5,
                "ambiguity": 0.7,  # High ambiguity on error
                "reversibility": 1.0,
                "user_confidence": 0.3,
            },
            "extraction_failed": True,
        }


def _parse_dimension_response(response: str) -> Dict[str, Any]:
    """Parse LLM response into dimension dict."""
    try:
        # Handle markdown-wrapped JSON
        text = response.strip()
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text:
            text = text.split("```")[1].split("```")[0].strip()
        
        dimensions = json.loads(text)
        
        # Validate structure
        if "a_set" not in dimensions:
            dimensions["a_set"] = {}
        if "b_set" not in dimensions:
            dimensions["b_set"] = {}
        
        return dimensions
        
    except (json.JSONDecodeError, KeyError) as e:
        logger.warning("Failed to parse dimension response: %s", str(e))
        return {"a_set": {}, "b_set": {}, "parse_error": str(e)}


def _calculate_completeness(a_set: dict) -> float:
    """Calculate A-set completeness (0.0-1.0)."""
    fields = ["what", "who", "when", "where", "how", "constraints"]
    filled = sum(1 for field in fields if a_set.get(field))
    return filled / len(fields)
```

**File to MODIFY:** `backend/dimensions/engine.py`

```python
# Add import at top
from dimensions.extractor import extract_dimensions_via_llm

class DimensionState:
    # ... existing code ...
    
    async def extract_and_update_via_llm(
        self,
        user_id: str,
        session_id: str,
        transcript: str,
        intent_summary: str,
        l1_suggestions: Dict[str, Any],
    ) -> None:
        """Extract dimensions via dedicated LLM call, then apply moving average.
        
        NEW: This replaces direct update from L1 suggestions.
        """
        # Call dedicated dimension extractor
        extracted = await extract_dimensions_via_llm(
            user_id=user_id,
            session_id=session_id,
            transcript=transcript,
            intent_summary=intent_summary,
            l1_suggestions=l1_suggestions,
        )
        
        # Now apply moving average (existing logic)
        self.update_from_suggestions(extracted.get("a_set", {}))
        
        # Update B-set with EMA
        b_set_extracted = extracted.get("b_set", {})
        for key in ["urgency", "emotional_load", "ambiguity", "user_confidence"]:
            if key in b_set_extracted:
                current_val = getattr(self.b_set, key, 0.5)
                new_val = float(b_set_extracted[key])
                setattr(
                    self.b_set,
                    key,
                    self._buffer.update(current_val, new_val)
                )
        
        self.turn_count += 1
```

**File to MODIFY:** `backend/prompting/call_sites.py`

```python
# Register new call site
CALL_SITES = {
    # ... existing ...
    "DIMENSION_EXTRACTOR": CallSite(
        call_site_id="DIMENSION_EXTRACTOR",
        allowed_purposes=[PromptPurpose.DIMENSIONS_EXTRACT],
        owner_module="dimensions.extractor",
        description="Dedicated dimension extraction from intent + memory",
    ),
}
```

**Acceptance Criteria:**
- ‚úÖ Dedicated dimension extraction LLM call implemented
- ‚úÖ Uses DIMENSIONS_EXTRACT purpose
- ‚úÖ Integrates Digital Self for entity resolution
- ‚úÖ Moving average still applied for stability
- ‚úÖ Dimension completeness improves 15-20%
- ‚úÖ A-set accuracy improves 10-15%

---

## üìä PHASE 1 SUMMARY

**Week 1-2 Deliverables:**
1. ‚úÖ Digital Self integrated into intent extraction
2. ‚úÖ Memory recall section generator created
3. ‚úÖ Onboarding wizard (mobile + backend)
4. ‚úÖ Landing page updated (no false advertising)
5. ‚úÖ Tests passing
6. ‚úÖ Documentation updated

**Week 3 Deliverable:**
1. ‚úÖ Dedicated dimension extraction implemented
2. ‚úÖ DIMENSIONS_EXTRACT purpose now used
3. ‚úÖ Dimension accuracy improved

**Expected Improvements After Phase 0-1:**
- Intent extraction accuracy: +30-40%
- Dimension completeness: +20%
- Ambiguity reduction: -50-60%
- User correction rate: -40%
- First-time user experience: Dramatically better

---

## üü° PHASE 2: CONTINUOUS IMPROVEMENT (Week 6-9)

### Detailed specification in:
`/app/MYNDLENS_DYNAMIC_OPTIMIZATION_SPEC.md`

**Summary of Phase 2:**
- Outcome tracking infrastructure (already started in Phase 1)
- Analytics engine (6 functions)
- Insight discovery (automated)
- User correction capture API
- Section effectiveness analyzer
- Optimal token budget discovery

**Effort:** 4 weeks  
**Priority:** P2

**Deliverables:**
- Analytics dashboard showing accuracy per purpose
- Automated insights generated hourly
- Section effectiveness known
- Recommendations for optimization
- Foundation for A/B testing

---

## üöÄ PHASE 3: SELF-OPTIMIZATION (Week 10-12)

### Detailed specification in:
`/app/MYNDLENS_DYNAMIC_OPTIMIZATION_SPEC.md` (Phase 3)

**Summary of Phase 3:**
- A/B testing framework
- Adaptive section generators
- Self-updating policy engine
- Background optimization job
- Continuous learning

**Effort:** 3 weeks  
**Priority:** P3

**Deliverables:**
- A/B experiments running
- Policies auto-update based on data
- Section content adapts to user expertise
- Fully autonomous optimization system

---

## üìã Complete Implementation Checklist

### Pre-Implementation (Day 0)

**Preparation:**
- [ ] Review all three verification reports
- [ ] Review optimization specification
- [ ] Allocate 2 developers for 12 weeks
- [ ] Set up MongoDB collections (see below)
- [ ] Create feature branch: `feature/critical-fixes`
- [ ] Schedule daily standups for first 2 weeks

**MongoDB Setup:**
```javascript
// New collections needed
db.createCollection("onboarding_profiles")
db.createCollection("prompt_outcomes")
db.createCollection("optimization_reports")
db.createCollection("prompt_insights")
db.createCollection("policy_updates")

// Indexes
db.onboarding_profiles.createIndex({"user_id": 1}, {unique: true})
db.prompt_outcomes.createIndex({"prompt_id": 1})
db.prompt_outcomes.createIndex({"purpose": 1, "accuracy_score": -1})
db.prompt_outcomes.createIndex({"user_id": 1, "created_at": -1})
db.prompt_outcomes.createIndex({"stable_hash": 1})
```

---

### Week 1: Digital Self Integration

**Day 1:**
- [ ] Create `memory_recall.py` section generator (4 hours)
- [ ] Write unit tests for memory_recall (2 hours)
- [ ] Register in registry.py (30 mins)

**Day 2:**
- [ ] Modify `l1/scout.py` - add memory recall (2 hours)
- [ ] Modify `l2/sentry.py` - add memory recall (2 hours)
- [ ] Test L1 with memory context (2 hours)

**Day 3:**
- [ ] Create integration test: intent with/without memory (3 hours)
- [ ] Verify prompt snapshots include memory section (1 hour)
- [ ] Fix any bugs (2 hours)

**Day 4:**
- [ ] Deploy to staging (1 hour)
- [ ] Manual testing with real user data (3 hours)
- [ ] Measure accuracy improvement (baseline vs. with memory) (2 hours)

**Day 5:**
- [ ] Code review and refinements (3 hours)
- [ ] Documentation update (2 hours)
- [ ] Deploy to production (1 hour)

**Week 1 Gate:**
- ‚úÖ Digital Self integrated
- ‚úÖ Memory recall working
- ‚úÖ Accuracy improved 20-30%
- ‚úÖ Tests passing

---

### Week 2: Onboarding Wizard

**Day 6:**
- [ ] Create backend `api/onboarding.py` (4 hours)
- [ ] Add onboarding router to server.py (30 mins)
- [ ] Write backend unit tests (2 hours)

**Day 7:**
- [ ] Create mobile `app/onboarding.tsx` - Steps 1-3 (5 hours)
- [ ] Styling and UX polish (2 hours)

**Day 8:**
- [ ] Complete mobile wizard - Steps 4-5 (4 hours)
- [ ] Add routing logic in _layout.tsx (2 hours)
- [ ] Mobile UI testing (1 hour)

**Day 9:**
- [ ] Integration testing: mobile ‚Üí backend ‚Üí Digital Self (4 hours)
- [ ] Test data persistence (2 hours)
- [ ] Fix bugs (2 hours)

**Day 10:**
- [ ] Deploy backend to staging (1 hour)
- [ ] Deploy mobile to TestFlight/internal (2 hours)
- [ ] User acceptance testing with 5 test users (3 hours)
- [ ] Measure onboarding completion rate (1 hour)

**Week 2 Gate:**
- ‚úÖ Onboarding wizard functional
- ‚úÖ 80%+ completion rate
- ‚úÖ Digital Self populated with 20-30 items
- ‚úÖ New user experience excellent

---

### Week 2 (Parallel): Landing Page Update

**Day 6:**
- [ ] Review three options for landing page (1 hour)
- [ ] Get stakeholder approval on approach (30 mins)
- [ ] Implement chosen option (2 hours)
- [ ] Deploy to production (30 mins)
- [ ] Monitor user feedback (ongoing)

**Landing Page Gate:**
- ‚úÖ No false advertising
- ‚úÖ Accurate feature description
- ‚úÖ No user complaints about missing features

---

### Week 3: Dimension Extraction Enhancement

**Day 11:**
- [ ] Create `dimensions/extractor.py` (4 hours)
- [ ] Register DIMENSION_EXTRACTOR call site (30 mins)
- [ ] Write unit tests (2 hours)

**Day 12:**
- [ ] Modify `dimensions/engine.py` - add LLM extraction method (3 hours)
- [ ] Integration testing (3 hours)

**Day 13:**
- [ ] Integrate into main flow (where to call extractor) (3 hours)
- [ ] Test dimension accuracy improvement (2 hours)
- [ ] Comparison: old vs. new approach (2 hours)

**Day 14:**
- [ ] Deploy to staging (1 hour)
- [ ] Collect 7 days of comparison data (ongoing)
- [ ] Bug fixes based on staging (3 hours)

**Day 15:**
- [ ] Code review (2 hours)
- [ ] Deploy to production (1 hour)
- [ ] Monitor metrics (ongoing)

**Week 3 Gate:**
- ‚úÖ Dedicated dimension extraction working
- ‚úÖ DIMENSIONS_EXTRACT purpose now used
- ‚úÖ A-set completeness +15-20%
- ‚úÖ Ambiguity score more accurate

---

### Week 4-5: Outcome Tracking (per Optimization Spec)

**Detailed plan in `/app/MYNDLENS_DYNAMIC_OPTIMIZATION_SPEC.md`**

**Summary:**
- Create outcome schema and tracking API
- Integrate into all LLM call sites
- Build analytics functions
- Create admin dashboard
- Capture user corrections

**Week 4-5 Gate:**
- ‚úÖ 100% prompt tracking operational
- ‚úÖ Analytics showing real data
- ‚úÖ 30+ days of outcome data collected
- ‚úÖ Insights being generated

---

## üìö Supporting Documentation Reference

### Document 1: Prompt System Review
**File:** `/app/MYNDLENS_PROMPT_SYSTEM_REVIEW.md` (600 lines)

**Contents:**
- Architecture analysis
- Current state assessment (42% dynamic)
- What's missing for self-optimization
- Phase-by-phase improvement roadmap

**Use for:** Understanding optimization path

---

### Document 2: Flow Verification Report
**File:** `/app/MYNDLENS_FLOW_VERIFICATION_REPORT.md` (800 lines)

**Contents:**
- Intent extraction flow analysis (60% effective)
- Digital Self integration gaps (0% functional)
- Dimension extraction assessment (85% effective)
- Surgical fixes with code examples

**Use for:** Understanding current implementation gaps

---

### Document 3: Agent Creation Verification
**File:** `/app/MYNDLENS_AGENT_CREATION_VERIFICATION.md` (600 lines)

**Contents:**
- Agent creation feature verification (0% implemented)
- Marketing vs. reality analysis
- Three implementation options
- Effort estimates

**Use for:** Understanding agent creation gap

---

### Document 4: Dynamic Optimization Spec
**File:** `/app/MYNDLENS_DYNAMIC_OPTIMIZATION_SPEC.md` (1,200 lines)

**Contents:**
- Complete outcome tracking implementation
- Analytics engine specification
- A/B testing framework
- Adaptive section generators
- Self-updating policy engine
- 16 surgical actions with production-ready code

**Use for:** Phase 2-3 implementation

---

### Document 5: Combined Codebase Analysis
**File:** `/app/COMBINED_CODEBASE_ANALYSIS.md` (400 lines)

**Contents:**
- ObeGee + MyndLens architecture overview
- Integration points
- Token feasibility analysis
- Review strategies

**Use for:** System-wide understanding

---

## üéØ Success Metrics

### Phase 0 Success (Week 1-2)

**Metrics to Track:**
- ‚úÖ Intent extraction accuracy: Baseline ‚Üí +30%
- ‚úÖ Ambiguity score: Baseline ‚Üí -50%
- ‚úÖ User correction rate: Baseline ‚Üí -40%
- ‚úÖ Onboarding completion rate: >80%
- ‚úÖ New user Digital Self items: 0 ‚Üí 20-30
- ‚úÖ Landing page accuracy: False ‚Üí True

### Phase 1 Success (Week 3-5)

**Metrics to Track:**
- ‚úÖ Dimension completeness: +15-20%
- ‚úÖ A-set accuracy: +10-15%
- ‚úÖ Prompt tracking coverage: 100%
- ‚úÖ Analytics dashboard operational
- ‚úÖ 30 days of outcome data collected

### Phase 2-3 Success (Week 6-12)

**Metrics to Track:**
- ‚úÖ Overall accuracy improvement: +25-35%
- ‚úÖ Section effectiveness scores available
- ‚úÖ A/B experiments running
- ‚úÖ Policies auto-updating
- ‚úÖ Continuous learning operational

---

## üî¨ Testing Strategy

### Unit Tests (Per Component)

**Files to Create:**
- `tests/test_memory_recall_section.py`
- `tests/test_digital_self_integration.py`
- `tests/test_onboarding_api.py`
- `tests/test_dimension_extractor.py`
- `tests/test_outcome_tracking.py`

**Coverage Target:** >80% for new code

### Integration Tests

**Scenarios to Test:**
1. ‚úÖ Intent with memory vs. without memory
2. ‚úÖ Onboarding ‚Üí Digital Self population ‚Üí Intent extraction
3. ‚úÖ Dimension extraction with Digital Self
4. ‚úÖ Outcome tracking end-to-end
5. ‚úÖ User correction capture

### E2E Tests

**User Journeys:**
1. ‚úÖ New user ‚Üí Onboarding ‚Üí First voice command ‚Üí Success
2. ‚úÖ User with memory ‚Üí Ambiguous command ‚Üí Resolved via Digital Self
3. ‚úÖ User correction ‚Üí Outcome tracked ‚Üí Analytics updated

---

## ‚ö†Ô∏è Risk Mitigation

### Risk 1: Breaking Changes

**Mitigation:**
- All changes are additive (not replacing existing code)
- Feature flags for gradual rollout
- Extensive testing before production
- Rollback plan for each change

### Risk 2: Performance Impact

**Mitigation:**
- Memory recall is async and fast (<50ms)
- Cache memory results per session
- Monitor latency before/after
- Optimize if degradation >10%

### Risk 3: Data Quality

**Mitigation:**
- Onboarding data validated before storage
- Provenance tracked for all facts
- User can review/edit Digital Self
- Export/delete functionality

### Risk 4: User Adoption

**Mitigation:**
- Onboarding wizard is optional (can skip)
- Clear value proposition shown
- Quick setup (2-3 minutes)
- Immediate benefit demonstrated

---

## üí∞ Effort Summary

### Phase 0 (Week 1-2) - CRITICAL

| Task | Effort | Priority |
|------|--------|----------|
| Digital Self Integration | 1 week | P0 |
| Onboarding Wizard | 1 week | P0 |
| Landing Page Update | 1 hour | P0 |
| **Total** | **2 weeks** | **URGENT** |

### Phase 1 (Week 3-5)

| Task | Effort | Priority |
|------|--------|----------|
| Dimension Extraction | 1 week | P1 |
| Outcome Tracking | 2 weeks | P1 |
| **Total** | **3 weeks** | **HIGH** |

### Phase 2-3 (Week 6-12)

| Task | Effort | Priority |
|------|--------|----------|
| Analytics Engine | 2 weeks | P2 |
| A/B Testing | 2 weeks | P2 |
| Self-Optimization | 2 weeks | P3 |
| **Total** | **6 weeks** | **MEDIUM** |

### Grand Total

**Critical Path:** 12 weeks  
**Minimum Viable:** 2 weeks (Phase 0 only)  
**Recommended:** 5 weeks (Phase 0 + Phase 1)

---

## üéØ Recommended Execution Plan

### Option A: Full Implementation (Recommended)

**Timeline:** 12 weeks  
**Team:** 2 developers  
**Outcome:** Complete, production-ready, self-optimizing system

**Schedule:**
- Weeks 1-2: Critical fixes (Phase 0)
- Weeks 3-5: Core functionality (Phase 1)
- Weeks 6-9: Analytics and insights (Phase 2)
- Weeks 10-12: Self-optimization (Phase 3)

### Option B: Critical Only (Minimum Viable)

**Timeline:** 2 weeks  
**Team:** 1-2 developers  
**Outcome:** Core promises fulfilled, no false advertising

**Schedule:**
- Week 1: Digital Self integration
- Week 2: Onboarding wizard + landing page update

**Then:** Assess user feedback, decide on Phase 1-3

### Option C: Accelerated (If Resources Available)

**Timeline:** 8 weeks  
**Team:** 3 developers  
**Outcome:** Faster delivery via parallelization

**Schedule:**
- Weeks 1-2: Phase 0 (Dev 1 + Dev 2)
- Weeks 3-4: Phase 1 Dimension + Tracking (parallel)
- Weeks 5-6: Phase 2 Analytics (Dev 1 + Dev 2)
- Weeks 7-8: Phase 3 A/B + Optimization (Dev 1 + Dev 2 + Dev 3)

---

## üö® Critical Decisions Required

### Decision 1: Agent Creation Feature

**Options:**

**A) Remove from marketing** ‚≠ê RECOMMENDED
- Effort: 1 hour
- Honest about capabilities
- No technical debt

**B) Build full agent creation** 
- Effort: 8-10 weeks additional
- Requires architecture changes in ObeGee + MyndLens
- High complexity

**C) Implement "virtual agents"**
- Effort: 3 weeks additional
- Policy-based simulation
- Lighter weight compromise

**Required By:** End of Week 1

### Decision 2: Onboarding Approach

**Options:**

**A) Full wizard** ‚≠ê RECOMMENDED
- 5-step comprehensive setup
- Collects contacts, preferences, tasks
- 2-3 minute completion time

**B) Minimal quick-setup**
- 1-step simplified version
- Just 3-5 contacts
- 30 second completion
- Can expand later

**C) Progressive onboarding**
- Collect data over first week
- Ask for one thing per day
- Lower initial friction

**Required By:** Start of Week 2

### Decision 3: Phased Rollout

**Options:**

**A) All at once** (after Phase 0)
- Deploy everything together
- Higher risk
- Faster to full capability

**B) Gradual rollout** ‚≠ê RECOMMENDED
- 10% ‚Üí 25% ‚Üí 50% ‚Üí 100% of users
- Monitor metrics at each stage
- Lower risk

**C) A/B cohorts**
- 50% get new features
- 50% stay on old system
- Compare outcomes
- Best for validation

**Required By:** End of Week 2

---

## üìû Questions for MyndLens Team

### Technical Questions

1. **Onboarding:** Voice-guided vs. form-based wizard?
2. **Data migration:** Any existing users with manual Digital Self data?
3. **Memory storage:** ChromaDB capacity planning for scale?
4. **Performance:** Acceptable latency increase from memory recall?

### Product Questions

5. **Agent creation:** Remove from marketing or build it?
6. **Timeline:** Can you commit 2 developers for 12 weeks?
7. **Priorities:** Phase 0 only, or continue to Phase 1-3?
8. **User communication:** How to message changes to existing users?

### Business Questions

9. **Budget:** Estimated infrastructure cost increase?
10. **Compliance:** Any regulatory concerns with storing user data?
11. **Privacy:** User data export/deletion workflows priority?

---

## üé¨ Conclusion

### Current State: **D (45/100)**

**What Works:**
- ‚úÖ Excellent architecture
- ‚úÖ L1+L2 validation pipeline
- ‚úÖ Dimension model (A-set+B-set)
- ‚úÖ Proper ObeGee integration

**What's Broken:**
- ‚ùå Digital Self not integrated (0% functional)
- ‚ùå No onboarding wizard (0% functional)
- ‚ùå Agent creation missing (false advertising)
- ‚ùå No outcome tracking (prevents improvement)
- ‚ùå 71% of advertised features missing

### After Phase 0: **B (82/100)**

**Improvements:**
- ‚úÖ Digital Self integrated
- ‚úÖ Onboarding wizard working
- ‚úÖ Honest marketing
- ‚úÖ Great new user experience
- ‚úÖ 30-40% accuracy improvement

### After Full Implementation: **A (95/100)**

**Capabilities:**
- ‚úÖ All core features working
- ‚úÖ Continuous learning enabled
- ‚úÖ Self-optimizing system
- ‚úÖ Competitive advantage from data
- ‚úÖ Industry-leading accuracy

---

## üìã Deliverables Summary

**This Document Provides:**
1. ‚úÖ Complete audit findings (5 critical issues)
2. ‚úÖ Detailed implementation plans (3 phases)
3. ‚úÖ Production-ready code examples (2,000+ lines)
4. ‚úÖ Testing strategies (unit + integration + E2E)
5. ‚úÖ Week-by-week execution plan (12 weeks)
6. ‚úÖ Success criteria per phase
7. ‚úÖ Risk mitigation strategies
8. ‚úÖ Decision framework
9. ‚úÖ References to 5 supporting documents

**Total Documentation:** 6 comprehensive specification documents, 4,000+ lines

**Code Provided:** 2,000+ lines of production-ready implementation code

**Everything needed to execute is included.**

---

## üöÄ Next Steps

**Immediate (Next 24 Hours):**
1. Review this master plan with stakeholders
2. Make Decision #1 (agent creation approach)
3. Allocate developer resources
4. Set up MongoDB collections
5. Create feature branch
6. Start Week 1 implementation

**This Week:**
- Complete Phase 0, Day 1-5 (Digital Self integration)

**Next Week:**
- Complete Phase 0, Day 6-10 (Onboarding wizard)

**Then:**
- Assess results
- Decide on Phase 1-3 continuation

---

**MyndLens team has everything needed to transform from 29% feature completion to 95%+ within 12 weeks.**

**The path is clear. The code is ready. Execution starts now.**
